---
layout: page_full
title: Tutorial 
background: '/img/bg-index.jpg'
---
<p><span style="font-size: 28pt; font-family: 'Arial', Helvetica, sans-serif;"><em>Tutorial (IEEE CEC 2022)</em></span></p>
<p><span style="font-size: 24pt; font-family: 'Arial', Helvetica, sans-serif;">How to Compare Evolutionary Multi-Objective Optimization Algorithms: Parameter Specifications, Indicators and Test Problems </span>
<br><span style="font-size: 18pt; font-family: 'Arial', Helvetica, sans-serif;"><em>The IEEE World Congress on Computational Intelligence 2022 </em></span>
<br><span style="font-size: 18pt; font-family: 'Arial', Helvetica, sans-serif;"><em>18-23 July, 2022, Padua, Italy </em></span><br></p>

  
<p><span style="font-size: 16pt; font-family: 'Arial', Helvetica, sans-serif;">Evolutionary multi-objective optimization (EMO) has been a very active research area in recent years. Almost every year, new EMO algorithms are proposed. When a new EMO algorithm is proposed, computational experiments are usually conducted in order to compare its performance with existing algorithms. 
Then, experimental results are summarized and reported as a number of tables together with statistical significance test results. Those results usually show higher performance of the new algorithm than existing algorithms. However, fair comparison of different EMO algorithms is not easy since the evaluated performance of each algorithm usually depends on experimental settings. 
This is also because solution sets instead of solutions are evaluated.</span></p>
