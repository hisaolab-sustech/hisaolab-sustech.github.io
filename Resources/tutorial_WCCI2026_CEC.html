---
layout: page_full
title: Tutorials 
background: '/img/bg-index.jpg'
---
<p><span style="color:red;font-size: 18pt;"><em><b> Tutorial (IEEE WCCI 2026)</b></em></span></p> <!--<a href ="https://2023.ieee-cec.org/tutorials/"> [Link] </a></b></em></span></p> --> 
<p><span style="font-size: 24pt; font-family: 'Arial', Helvetica, sans-serif;">Fair Performance Comparison of Evolutionary Multi-Objective Algorithms </span>
<br><span style="font-size: 18pt; font-family: 'Arial', Helvetica, sans-serif;"><em>IEEE WCCI 2026 (CEC)</em></span>
<br><span style="font-size: 18pt; font-family: 'Arial', Helvetica, sans-serif;"><em>21-26 June, 2026, Maastricht, the Netherlands </em></span><br></p>
<img src="/Resources/homepage-banner_boven-WCCI-1.jpg" style="width:524px;height:298px;" class="center">

<p><span style="font-size: 20pt; font-family: 'Arial', Helvetica, sans-serif;"><b>Short introduction: </b></span><br>  
<span style="font-size: 16pt; font-family: 'Arial', Helvetica, sans-serif;">Evolutionary multi-objective optimization (EMO) has been a very active research area in recent years. 
  Almost every year, new EMO algorithms are proposed. When a new EMO algorithm is proposed, computational experiments are usually conducted in order to compare its performance with existing algorithms. 
  Then, experimental results are summarized and reported as a number of tables together with statistical significance test results. Those results usually show higher performance of the new algorithm than existing algorithms. 
  However, fair comparison of different EMO algorithms is not easy since the evaluated performance of each algorithm usually depends on experimental settings. This is also because solution sets instead of solutions are evaluated. 
  In this tutorial, we will first explain some commonly-used software platforms and experimental settings for the comparison of EMO algorithms. 
  Then, we will discuss how to specify the common setting of computational experiments, which are used by all the compared EMO algorithms. 
  More specifically, the focus of this tutorial is the setting related to the following four issues: (i) termination condition, (ii) population size, (iii) performance indicators, (iv) test problem. 
  For each issue, we will provide a clear demonstration of its strong effects on comparison results of EMO algorithms. Following that, we will discuss how to handle each of these issues for fair comparison. 
  These discussions aim to encourage the future development of the EMO research field without focusing too much on the development of overly-specialized new algorithms in a specific setting. 
  Finally, we will also suggest some promising future research topics related to each issue.</span></p>

<p><span style="font-size: 20pt; font-family: 'Arial', Helvetica, sans-serif;"><b>Outline of the tutorial: </b></span><br>
<span style="font-size: 16pt; font-family: 'Arial', Helvetica, sans-serif;">The duration of the tutorial will be two hours. It will be composed of the following parts:<br>
1.	Intro: Why fair EMO comparisons are challenging and difficult <br>
2.	Basics: EMO Platforms & common settings <br>
3.	Demos: how the four key settings (termination condition, population size, performance indicators, test problems) bias comparison results <br>
4.	Takeaways + recommended settings for fair comparisons <br>
5.  Q&A </span></p>

 <p><span style="font-size: 20pt; font-family: 'Arial', Helvetica, sans-serif;"><b>Speakers: </b></span><br>
 <span style="font-size: 16pt; font-family: 'Arial', Helvetica, sans-serif;"><b>Lie Meng Pang, Southern University of Science and Technology, China.</b><br>
  Lie Meng Pang received her Bachelor of Engineering degree in Electronic and Telecommunication Engineering and Ph.D. degree in Electronic Engineering from the Faculty of Engineering, Universiti Malaysia Sarawak, Malaysia, in 2012 and 2018, respectively. 
   She is currently a research associate with the Department of Computer Science and Engineering, Southern University of Science and Technology (SUSTech), China. 
  Her current research interests include evolutionary multi-objective optimization and fuzzy systems. 
   She is a Senior Member of the IEEE and a member of the IEEE Computational Intelligence Society (CIS) Evolutionary Computation Technical Committee. She received the Best Paper Award at EMO 2025.</p>

 <span style="font-size: 16pt; font-family: 'Arial', Helvetica, sans-serif;"><b>Hisao Ishibuchi, Southern University of Science and Technology, China.</b><br>
 Hisao Ishibuchi is a Chair Professor at Southern University of Science and Technology, China. He was the IEEE Computational Intelligence Society (CIS) Vice-President for Technical Activities in 2010-2013 and the Editor-in-Chief of IEEE Computational Intelligence Magazine in 2014-2019. 
   Currently he is an IEEE CIS Administrative Committee Member, an IEEE CIS Distinguished Lecturer, and an Associate Editor of several journals such as IEEE Transactions on Cybernetics and ACM Computing Surveys. 
   He is/was General Chair of EMO 2027, IEEE WCCI 2024 and EMO 2021, and Program Chair of FUZZ-IEEE 2026, IEEE SSCI 2023, and IEEE CEC 2010. He is also Workshops Chair of IEEE CAI 2026, Area Chair of NeurIPS 2025, and Senior Program Committee Member of AAAI 2026. 
   He received a Fuzzy Systems Pioneer Award from IEEE CIS in 2019, an Outstanding Paper Award from IEEE Transactions on Evolutionary Computation in 2020, and Best Paper Awards from FUZZ-IEEE 2009, 2011, EMO 2019, 2025, and GECCO 2004, 2017, 2018, 2020, 2021, 2024. 
   He also received a JSPS prize in 2007. He is an IEEE Fellow.</p>
